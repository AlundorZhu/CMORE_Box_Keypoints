{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df88fd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(label_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     28\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(label_data)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mgenerate_train_frames_and_labels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvidPath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./IMG_1156.mov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_txt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./IMG_1156.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_frame_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/train/images\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_label_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/train/labels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     36\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 24\u001b[0m, in \u001b[0;36mgenerate_train_frames_and_labels\u001b[1;34m(vidPath, label_txt_path, save_frame_path, save_label_path)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     23\u001b[0m frame_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_frame_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfileName\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_frame_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(frame_idx)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m06d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m label_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_label_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfileName\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_frame_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(frame_idx)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m06d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(label_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\evs271_hw\\lib\\site-packages\\ultralytics\\utils\\patches.py:65\u001b[0m, in \u001b[0;36mimwrite\u001b[1;34m(filename, img, params)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Write an image to a file with multilanguage filename support.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtofile(filename)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "\n",
    "def generate_train_frames_and_labels(vidPath, label_txt_path, save_frame_path, save_label_path):\n",
    "    if not os.path.exists(label_txt_path):\n",
    "        return\n",
    "\n",
    "    os.makedirs(save_frame_path, exist_ok=True)\n",
    "    os.makedirs(save_label_path, exist_ok=True)\n",
    "                   \n",
    "    fileName = vidPath.split('/')[-1]\n",
    "    fileName = fileName.split('.')[0]\n",
    "    \n",
    "    cap = cv.VideoCapture(vidPath)\n",
    "    with open(label_txt_path, 'r') as f:\n",
    "            label_data = f.read()\n",
    "    while cap.isOpened():\n",
    "        frame_idx = cap.get(cv.CAP_PROP_POS_FRAMES)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_path = os.path.join(save_frame_path, f\"{fileName}_frame_{int(frame_idx):06d}.jpg\")\n",
    "        cv.imwrite(frame_path, frame)\n",
    "                                                                                                                        \n",
    "        label_path = os.path.join(save_label_path, f\"{fileName}_frame_{int(frame_idx):06d}.txt\")\n",
    "        with open(label_path, 'w') as f:\n",
    "            f.write(label_data)\n",
    "            \n",
    "\n",
    "generate_train_frames_and_labels(\n",
    "    vidPath='./IMG_1156.mov',\n",
    "    label_txt_path='./IMG_1156.txt',\n",
    "    save_frame_path='data/train/images',\n",
    "    save_label_path='data/train/labels'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d82b0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_yolo_label(label_line: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse a single YOLO pose label line\n",
    "    \n",
    "    Args:\n",
    "        label_line: String like \"0 0.538 0.468 0.780 0.643 0.148 0.639 2 ...\"\n",
    "    \n",
    "    Returns:\n",
    "        dict with:\n",
    "            - class_id: int\n",
    "            - bbox: np.array shape (4,) [x_center, y_center, width, height]\n",
    "            - keypoints_xy: np.array shape (10, 2) [x, y coordinates]\n",
    "            - visibility: np.array shape (10,) [visibility flags]\n",
    "    \"\"\"\n",
    "    values = label_line.strip().split()\n",
    "    values = [float(v) for v in values]\n",
    "    \n",
    "    class_id = int(values[0])\n",
    "    bbox = np.array(values[1:5])\n",
    "    \n",
    "    # Keypoints: groups of 3 (x, y, visibility)\n",
    "    kpt_data = np.array(values[5:]).reshape(-1, 3)\n",
    "    keypoints_xy = kpt_data[:, :2]  # Just x, y\n",
    "    visibility = kpt_data[:, 2]      # Visibility flags\n",
    "    \n",
    "    return {\n",
    "        'class_id': class_id,\n",
    "        'bbox': bbox,\n",
    "        'keypoints_xy': keypoints_xy,\n",
    "        'visibility': visibility\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_yolo_label_file(label_path: str) -> Dict:\n",
    "    \"\"\"Parse label from a .txt file\"\"\"\n",
    "    with open(label_path, 'r') as f:\n",
    "        line = f.read().strip()\n",
    "    return parse_yolo_label(line)\n",
    "\n",
    "\n",
    "def get_yolo_predictions(model, image_path: str, normalize: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Run YOLO inference and extract keypoints\n",
    "    \n",
    "    Args:\n",
    "        model: YOLO model instance\n",
    "        image_path: Path to image\n",
    "        normalize: If True, normalize keypoints to [0, 1] range\n",
    "    Returns:\n",
    "        dict with:\n",
    "            - keypoints_xy: np.array shape (10, 2)\n",
    "            - confidence: float (detection confidence)\n",
    "    \"\"\"\n",
    "    from ultralytics import YOLO\n",
    "    \n",
    "    results = model.predict(image_path)\n",
    "    \n",
    "    \n",
    "    image = cv.imread(str(image_path))\n",
    "    img_height, img_width = image.shape[:2]\n",
    "    # print(f\"Image size: width={img_width}, height={img_height}\")\n",
    "    \n",
    "    # Check if any detections\n",
    "    if len(results[0].keypoints.xy) == 0:\n",
    "        # No detection - return zeros\n",
    "        return {\n",
    "            'keypoints_xy': np.zeros((10, 2)),\n",
    "            'confidence': 0.0\n",
    "        }\n",
    "    \n",
    "    # Get first detection (assuming single box in image)\n",
    "    pred_keypoints = results[0].keypoints.xy[0].cpu().numpy()\n",
    "    \n",
    "    if normalize:\n",
    "        # print(\"Normalizing predicted keypoints\")\n",
    "        # print(f\"Predicted keypoints before normalization: {pred_keypoints}\")\n",
    "        pred_keypoints_normalized = pred_keypoints.copy()\n",
    "        pred_keypoints_normalized[:, 0] = pred_keypoints_normalized[:, 0] / img_width   # Normalize x\n",
    "        pred_keypoints_normalized[:, 1] = pred_keypoints_normalized[:, 1] / img_height  # Normalize y\n",
    "        # print(f\"Predicted keypoints after normalization: {pred_keypoints_normalized}\")\n",
    "    else:\n",
    "        pred_keypoints_normalized = pred_keypoints\n",
    "    \n",
    "    # Get detection confidence if available\n",
    "    confidence = float(results[0].boxes.conf[0]) if len(results[0].boxes.conf) > 0 else 1.0\n",
    "    \n",
    "    return {\n",
    "        'keypoints_xy': pred_keypoints_normalized,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "def calculate_euclidean_distance(pred: np.ndarray, gt: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance between predicted and ground truth keypoints\n",
    "    \n",
    "    Args:\n",
    "        pred: shape (K, 2) - K keypoints, (x, y) coordinates\n",
    "        gt: shape (K, 2)\n",
    "    \n",
    "    Returns:\n",
    "        distances: shape (K,) - distance for each keypoint\n",
    "    \"\"\"\n",
    "    diff = pred - gt\n",
    "    distances = np.sqrt(np.sum(diff ** 2, axis=1))\n",
    "    return distances\n",
    "\n",
    "def calculate_mse_single(pred: np.ndarray, gt: np.ndarray, \n",
    "                        visibility: np.ndarray = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate MSE (Mean Squared Error) for a single image\n",
    "    \n",
    "    Args:\n",
    "        pred: shape (K, 2) - predicted keypoints\n",
    "        gt: shape (K, 2) - ground truth keypoints\n",
    "        visibility: shape (K,) - visibility flags (optional)\n",
    "    \n",
    "    Returns:\n",
    "        dict with:\n",
    "            - mse_overall: float (average MSE across all keypoints)\n",
    "            - mse_per_keypoint: np.array shape (K,)\n",
    "    \"\"\"\n",
    "    squared_diff = (pred - gt) ** 2\n",
    "    \n",
    "    if visibility is not None:\n",
    "        # Only compute MSE for visible keypoints (visibility == 2)\n",
    "        visible_mask = (visibility == 2)\n",
    "        \n",
    "        if not np.any(visible_mask):\n",
    "            # No visible keypoints\n",
    "            return {\n",
    "                'mse_overall': float('inf'),\n",
    "                'mse_per_keypoint': np.full(len(pred), float('inf'))\n",
    "            }\n",
    "        \n",
    "        # MSE per keypoint (x and y averaged)\n",
    "        mse_per_kpt = np.mean(squared_diff, axis=1)\n",
    "        mse_per_kpt = np.where(visible_mask, mse_per_kpt, np.nan)\n",
    "        \n",
    "        # Overall MSE (only visible)\n",
    "        mse_overall = np.nanmean(mse_per_kpt)\n",
    "    else:\n",
    "        mse_per_kpt = np.mean(squared_diff, axis=1)\n",
    "        mse_overall = np.mean(mse_per_kpt)\n",
    "    \n",
    "    return {\n",
    "        'mse_overall': float(mse_overall),\n",
    "        'mse_per_keypoint': mse_per_kpt\n",
    "    }\n",
    "\n",
    "def calculate_mae_single(pred: np.ndarray, gt: np.ndarray,\n",
    "                        visibility: np.ndarray = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate MAE (Mean Absolute Error) for a single image\n",
    "    \n",
    "    Args:\n",
    "        pred: shape (K, 2)\n",
    "        gt: shape (K, 2)\n",
    "        visibility: shape (K,)\n",
    "    \n",
    "    Returns:\n",
    "        dict with:\n",
    "            - mae_overall: float\n",
    "            - mae_per_keypoint: np.array shape (K,)\n",
    "    \"\"\"\n",
    "    abs_diff = np.abs(pred - gt)\n",
    "    \n",
    "    if visibility is not None:\n",
    "        visible_mask = (visibility == 2)\n",
    "        \n",
    "        if not np.any(visible_mask):\n",
    "            return {\n",
    "                'mae_overall': float('inf'),\n",
    "                'mae_per_keypoint': np.full(len(pred), float('inf'))\n",
    "            }\n",
    "        \n",
    "        mae_per_kpt = np.mean(abs_diff, axis=1)\n",
    "        mae_per_kpt = np.where(visible_mask, mae_per_kpt, np.nan)\n",
    "        mae_overall = np.nanmean(mae_per_kpt)\n",
    "    else:\n",
    "        mae_per_kpt = np.mean(abs_diff, axis=1)\n",
    "        mae_overall = np.mean(mae_per_kpt)\n",
    "    \n",
    "    return {\n",
    "        'mae_overall': float(mae_overall),\n",
    "        'mae_per_keypoint': mae_per_kpt\n",
    "    }\n",
    "    \n",
    "def calculate_pck_single(pred: np.ndarray, gt: np.ndarray, \n",
    "                        threshold: float = 0.05) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate PCK (Percentage of Correct Keypoints) for a single image\n",
    "    \n",
    "    Args:\n",
    "        pred: shape (K, 2)\n",
    "        gt: shape (K, 2)\n",
    "        threshold: distance threshold (normalized, e.g., 0.05 = 5% of image)\n",
    "    \n",
    "    Returns:\n",
    "        dict with:\n",
    "            - pck: float (percentage of correct keypoints, 0-1)\n",
    "            - correct_per_keypoint: np.array shape (K,) - bool array\n",
    "    \n",
    "    Interpretation:\n",
    "        PCK@0.05 = 0.95 means 95% of keypoints within 5% of image size    \n",
    "    \"\"\"\n",
    "    distances = calculate_euclidean_distance(pred, gt)\n",
    "    correct = distances < threshold\n",
    "    pck = np.mean(correct)\n",
    "    \n",
    "    return {\n",
    "        'pck': float(pck),\n",
    "        'correct_per_keypoint': correct\n",
    "    }\n",
    "\n",
    "def evaluate_single_frame(pred_keypoints: np.ndarray, \n",
    "                               gt_keypoints: np.ndarray,\n",
    "                               visibility: np.ndarray = None,\n",
    "                               thresholds: list = [0.02, 0.05, 0.10]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate ALL metrics for a single frame\n",
    "    \n",
    "    Args:\n",
    "        pred_keypoints: shape (K, 2) - predicted keypoints\n",
    "        gt_keypoints: shape (K, 2) - ground truth keypoints\n",
    "        visibility: shape (K,) - visibility flags\n",
    "        thresholds: list of PCK thresholds to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        dict with all metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # MSE\n",
    "    mse_result = calculate_mse_single(pred_keypoints, gt_keypoints, visibility)\n",
    "    metrics['mse_overall'] = mse_result['mse_overall']\n",
    "    metrics['mse_per_keypoint'] = mse_result['mse_per_keypoint']\n",
    "    \n",
    "    # MAE\n",
    "    mae_result = calculate_mae_single(pred_keypoints, gt_keypoints, visibility)\n",
    "    metrics['mae_overall'] = mae_result['mae_overall']\n",
    "    metrics['mae_per_keypoint'] = mae_result['mae_per_keypoint']\n",
    "    \n",
    "    # PCK at multiple thresholds\n",
    "    for thresh in thresholds:\n",
    "        pck_result = calculate_pck_single(pred_keypoints, gt_keypoints, thresh)\n",
    "        metrics[f'pck_{thresh}'] = pck_result['pck']\n",
    "        metrics[f'pck_{thresh}_per_keypoint'] = pck_result['correct_per_keypoint']\n",
    "    \n",
    "    # Per-keypoint Euclidean distances (useful for analysis)\n",
    "    metrics['distances'] = calculate_euclidean_distance(pred_keypoints, gt_keypoints)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c98edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SINGLE IMAGE EVALUATION EXAMPLE\n",
      "\n",
      "Loaded model: .//best.pt\n",
      "Image: data/train/images/IMG_1156_frame_000001.png\n",
      "Label: data/train/labels/IMG_1156_frame_000001.txt\n",
      "\n",
      "Ground Truth Keypoints:\n",
      "{'class_id': 0, 'bbox': array([    0.48884,     0.43644,     0.90866,     0.67783]), 'keypoints_xy': array([[   0.034516,     0.57294],\n",
      "       [   0.064146,     0.77364],\n",
      "       [    0.49159,     0.55046],\n",
      "       [    0.49159,     0.77469],\n",
      "       [    0.94317,     0.57562],\n",
      "       [    0.91748,     0.77536],\n",
      "       [    0.49115,    0.097528],\n",
      "       [    0.48891,     0.28708],\n",
      "       [    0.17536,     0.28119],\n",
      "       [    0.80877,      0.2842]]), 'visibility': array([          2,           2,           2,           2,           2,           2,           2,           2,           2,           2])}\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000001.png: 288x512 1 box, 13.4ms\n",
      "Speed: 2.8ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 512)\n",
      "pred_data: {'keypoints_xy': array([[   0.036935,     0.57035],\n",
      "       [   0.056353,     0.78084],\n",
      "       [    0.48902,     0.53869],\n",
      "       [    0.49609,      0.7691],\n",
      "       [     0.9398,     0.56567],\n",
      "       [    0.94053,     0.77494],\n",
      "       [    0.49529,    0.086204],\n",
      "       [    0.48078,     0.26635],\n",
      "       [     0.1669,     0.29803],\n",
      "       [    0.84586,     0.28604]], dtype=float32), 'confidence': 0.9663413166999817}\n",
      "\n",
      "Overall Metrics:\n",
      "  MSE: 0.000167\n",
      "  MAE: 0.009490\n",
      "  PCK@0.02: 0.7000\n",
      "  PCK@0.05: 1.0000\n",
      "  PCK@0.10: 1.0000\n",
      "\n",
      "Per-Keypoint Analysis:\n",
      "  Keypoint     Distance     MSE          Correct@0.05\n",
      "  --------------------------------------------------\n",
      "  FrontTopL    0.003546     0.000006     Yes\n",
      "  FrontBotL    0.010608     0.000056     Yes\n",
      "  FrontTopM    0.012054     0.000073     Yes\n",
      "  FrontBotM    0.007179     0.000026     Yes\n",
      "  FrontTopR    0.010508     0.000055     Yes\n",
      "  FrontBotR    0.023058     0.000266     Yes\n",
      "  BackDivTop   0.012059     0.000073     Yes\n",
      "  FrontDivTop  0.022273     0.000248     Yes\n",
      "  BackTopL     0.018840     0.000177     Yes\n",
      "  BackTopR     0.037129     0.000689     Yes\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"SINGLE IMAGE EVALUATION EXAMPLE\")\n",
    "\n",
    "# 1. Load model\n",
    "model_path = './/best.pt'\n",
    "model = YOLO(model_path)\n",
    "print(f\"\\nLoaded model: {model_path}\")\n",
    "\n",
    "# 2. Load image and label\n",
    "image_path = 'data/train/images/IMG_1156_frame_000001.png'\n",
    "label_path = 'data/train/labels/IMG_1156_frame_000001.txt'\n",
    "\n",
    "print(f\"Image: {image_path}\")\n",
    "print(f\"Label: {label_path}\")\n",
    "\n",
    "# 3. Parse ground truth\n",
    "gt_data = parse_yolo_label_file(label_path)\n",
    "\n",
    "print(\"\\nGround Truth Keypoints:\")\n",
    "print(gt_data)\n",
    "gt_keypoints = gt_data['keypoints_xy']\n",
    "visibility = gt_data['visibility']\n",
    "\n",
    "# 4. Get predictions\n",
    "pred_data = get_yolo_predictions(model, image_path, normalize=True)\n",
    "\n",
    "print(f\"pred_data: {pred_data}\")\n",
    "pred_keypoints = pred_data['keypoints_xy']\n",
    "confidence = pred_data['confidence']\n",
    "\n",
    "# 5. Calculate metrics\n",
    "metrics = evaluate_single_frame(\n",
    "    pred_keypoints, \n",
    "    gt_keypoints, \n",
    "    visibility,\n",
    "    thresholds=[0.02, 0.05, 0.10]\n",
    ")\n",
    "\n",
    "# 6. Display results\n",
    "print(f\"\\nOverall Metrics:\")\n",
    "print(f\"  MSE: {metrics['mse_overall']:.6f}\")\n",
    "print(f\"  MAE: {metrics['mae_overall']:.6f}\")\n",
    "print(f\"  PCK@0.02: {metrics['pck_0.02']:.4f}\")\n",
    "print(f\"  PCK@0.05: {metrics['pck_0.05']:.4f}\")\n",
    "print(f\"  PCK@0.10: {metrics['pck_0.1']:.4f}\")\n",
    "\n",
    "print(f\"\\nPer-Keypoint Analysis:\")\n",
    "print(f\"  {'Keypoint':<12} {'Distance':<12} {'MSE':<12} {'Correct@0.05'}\")\n",
    "print(f\"  {'-'*50}\")\n",
    "\n",
    "keypoint_names = [\n",
    "    'FrontTopL', 'FrontBotL', 'FrontTopM', 'FrontBotM',\n",
    "    'FrontTopR', 'FrontBotR', 'BackDivTop', 'FrontDivTop',\n",
    "    'BackTopL', 'BackTopR'\n",
    "]\n",
    "\n",
    "for i, name in enumerate(keypoint_names):\n",
    "    dist = metrics['distances'][i]\n",
    "    mse = metrics['mse_per_keypoint'][i]\n",
    "    correct = 'Yes' if metrics['pck_0.05_per_keypoint'][i] else 'No'\n",
    "    print(f\"  {name:<12} {dist:<12.6f} {mse:<12.6f} {correct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd4aa15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "\n",
    "def benchmark_resource_usage(model, test_images: List, num_warmup: int = 5, num_iterations: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Benchmark specifically designed for CPU-bound, resource-constrained environments.\n",
    "    Tracks End-to-End latency (Disk I/O + Inference) and Peak RAM usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    process = psutil.Process()\n",
    "    \n",
    "    # Prepare image list\n",
    "    benchmark_images = test_images[:min(num_iterations, len(test_images))]\n",
    "    # Handle tuple unpacking if your list is [(path, label), ...]\n",
    "    if len(benchmark_images) > 0 and isinstance(benchmark_images[0], tuple):\n",
    "        benchmark_images = [img for img, _ in benchmark_images]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CPU RESOURCE BENCHMARK (End-to-End)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # --- BASELINE MEMORY ---\n",
    "    # Force garbage collection before starting to get a clean baseline\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    baseline_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "    print(f\"Baseline Process RAM (Model Loaded): {baseline_memory_mb:.2f} MB\")\n",
    "\n",
    "    # --- WARMUP ---\n",
    "    # Essential to cache file system reads if OS caching is active\n",
    "    print(f\"\\nWarming up ({num_warmup} iterations)...\")\n",
    "    for i in range(num_warmup):\n",
    "        _ = model.predict(str(benchmark_images[i % len(benchmark_images)]), verbose=False)\n",
    "\n",
    "    # --- BENCHMARK ---\n",
    "    print(f\"Benchmarking ({len(benchmark_images)} iterations)...\")\n",
    "    \n",
    "    inference_times = []\n",
    "    peak_ram_mb = baseline_memory_mb\n",
    "    cpu_percentages = []\n",
    "\n",
    "    for img_path in benchmark_images:\n",
    "        # Track CPU usage during this specific interval\n",
    "        process.cpu_percent(interval=None) # Reset counter\n",
    "        \n",
    "        # High precision CPU timer\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Run End-to-End (Load -> Preprocess -> Infer -> Postprocess)\n",
    "        _ = model.predict(str(img_path), verbose=False, device='cpu')\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        # Measure metrics\n",
    "        current_mem = process.memory_info().rss / (1024 * 1024)\n",
    "        if current_mem > peak_ram_mb:\n",
    "            peak_ram_mb = current_mem\n",
    "            \n",
    "        inference_times.append((end_time - start_time) * 1000) # ms\n",
    "        \n",
    "        # Note: cpu_percent needs a blocking interval to be accurate, \n",
    "        # but since prediction is blocking, we can just poll it immediately after\n",
    "        # to see the average usage since the last call.\n",
    "        cpu_percentages.append(process.cpu_percent(interval=None))\n",
    "\n",
    "    # --- STATISTICS ---\n",
    "    inference_times = np.array(inference_times)\n",
    "    \n",
    "    results = {\n",
    "        # Timing\n",
    "        'avg_latency_ms': float(np.mean(inference_times)),\n",
    "        'p95_latency_ms': float(np.percentile(inference_times, 95)),\n",
    "        'fps': float(1000.0 / np.mean(inference_times)),\n",
    "        \n",
    "        # Resource Constraints Metrics\n",
    "        'baseline_ram_mb': float(baseline_memory_mb),\n",
    "        'peak_ram_mb': float(peak_ram_mb),\n",
    "        'avg_cpu_usage_percent': float(np.mean(cpu_percentages)),\n",
    "        \n",
    "        # Info\n",
    "        'num_samples': len(benchmark_images)\n",
    "    }\n",
    "\n",
    "    # --- REPORT ---\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"SUMMARY RESULTS\")\n",
    "    print(\"-\"*-70)\n",
    "    print(f\"Avg Latency (E2E):  {results['avg_latency_ms']:.2f} ms\")\n",
    "    print(f\"FPS:                {results['fps']:.2f}\")\n",
    "    print(f\"CPU Usage (Avg):    {results['avg_cpu_usage_percent']:.1f}%\")\n",
    "    print(\"-\"*-70)\n",
    "    print(f\"Baseline RAM:       {results['baseline_ram_mb']:.2f} MB\")\n",
    "    print(f\"Peak RAM:           {results['peak_ram_mb']:.2f} MB\")    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e7ec5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATE ALL TEST IMAGES\n",
      "Loading model from: .//best.pt\n",
      "Found 50 test images.\n",
      "Evaluating 50 images with labels.\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000000.png: 288x512 1 box, 13.3ms\n",
      "Speed: 0.8ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000001.png: 288x512 1 box, 12.5ms\n",
      "Speed: 0.9ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000002.png: 288x512 1 box, 13.1ms\n",
      "Speed: 0.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000003.png: 288x512 1 box, 12.6ms\n",
      "Speed: 0.8ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000004.png: 288x512 1 box, 11.9ms\n",
      "Speed: 0.9ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000005.png: 288x512 1 box, 13.1ms\n",
      "Speed: 0.8ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000006.png: 288x512 1 box, 12.0ms\n",
      "Speed: 0.8ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000007.png: 288x512 1 box, 12.9ms\n",
      "Speed: 0.8ms preprocess, 12.9ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000008.png: 288x512 1 box, 14.5ms\n",
      "Speed: 0.8ms preprocess, 14.5ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000009.png: 288x512 1 box, 12.3ms\n",
      "Speed: 0.8ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000010.png: 288x512 1 box, 12.2ms\n",
      "Speed: 0.7ms preprocess, 12.2ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000011.png: 288x512 1 box, 12.3ms\n",
      "Speed: 0.7ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000012.png: 288x512 1 box, 12.5ms\n",
      "Speed: 0.7ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000013.png: 288x512 1 box, 13.6ms\n",
      "Speed: 0.9ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000014.png: 288x512 1 box, 16.9ms\n",
      "Speed: 0.9ms preprocess, 16.9ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000015.png: 288x512 1 box, 12.5ms\n",
      "Speed: 0.8ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000016.png: 288x512 1 box, 13.9ms\n",
      "Speed: 1.0ms preprocess, 13.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000017.png: 288x512 1 box, 12.0ms\n",
      "Speed: 0.8ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000018.png: 288x512 1 box, 12.3ms\n",
      "Speed: 0.8ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000019.png: 288x512 1 box, 11.5ms\n",
      "Speed: 0.9ms preprocess, 11.5ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000020.png: 288x512 1 box, 14.3ms\n",
      "Speed: 0.8ms preprocess, 14.3ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000021.png: 288x512 1 box, 14.5ms\n",
      "Speed: 0.7ms preprocess, 14.5ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000022.png: 288x512 1 box, 12.3ms\n",
      "Speed: 0.8ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000023.png: 288x512 1 box, 12.6ms\n",
      "Speed: 0.8ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "  Progress: 25/50\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000024.png: 288x512 1 box, 12.3ms\n",
      "Speed: 0.9ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000025.png: 288x512 1 box, 12.5ms\n",
      "Speed: 0.8ms preprocess, 12.5ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000026.png: 288x512 1 box, 12.3ms\n",
      "Speed: 0.7ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000027.png: 288x512 1 box, 15.9ms\n",
      "Speed: 0.7ms preprocess, 15.9ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000028.png: 288x512 1 box, 14.9ms\n",
      "Speed: 0.8ms preprocess, 14.9ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000029.png: 288x512 1 box, 15.7ms\n",
      "Speed: 0.8ms preprocess, 15.7ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000030.png: 288x512 1 box, 15.7ms\n",
      "Speed: 0.7ms preprocess, 15.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000031.png: 288x512 1 box, 19.1ms\n",
      "Speed: 0.8ms preprocess, 19.1ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000032.png: 288x512 1 box, 14.9ms\n",
      "Speed: 0.7ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000033.png: 288x512 1 box, 16.0ms\n",
      "Speed: 0.8ms preprocess, 16.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000034.png: 288x512 1 box, 15.3ms\n",
      "Speed: 0.9ms preprocess, 15.3ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000035.png: 288x512 1 box, 15.4ms\n",
      "Speed: 0.7ms preprocess, 15.4ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000036.png: 288x512 1 box, 16.2ms\n",
      "Speed: 0.7ms preprocess, 16.2ms inference, 3.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000037.png: 288x512 1 box, 16.2ms\n",
      "Speed: 1.2ms preprocess, 16.2ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000038.png: 288x512 1 box, 19.8ms\n",
      "Speed: 0.9ms preprocess, 19.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000039.png: 288x512 1 box, 16.1ms\n",
      "Speed: 0.7ms preprocess, 16.1ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000040.png: 288x512 1 box, 16.1ms\n",
      "Speed: 0.9ms preprocess, 16.1ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000041.png: 288x512 1 box, 19.7ms\n",
      "Speed: 0.8ms preprocess, 19.7ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000042.png: 288x512 1 box, 16.1ms\n",
      "Speed: 0.7ms preprocess, 16.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000043.png: 288x512 1 box, 16.2ms\n",
      "Speed: 0.8ms preprocess, 16.2ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000044.png: 288x512 1 box, 15.9ms\n",
      "Speed: 0.7ms preprocess, 15.9ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000045.png: 288x512 1 box, 17.0ms\n",
      "Speed: 0.8ms preprocess, 17.0ms inference, 3.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000046.png: 288x512 1 box, 15.6ms\n",
      "Speed: 0.8ms preprocess, 15.6ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000047.png: 288x512 1 box, 16.8ms\n",
      "Speed: 0.9ms preprocess, 16.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000048.png: 288x512 1 box, 16.9ms\n",
      "Speed: 0.8ms preprocess, 16.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 512)\n",
      "  Progress: 50/50\n",
      "\n",
      "image 1/1 d:\\cs_projects\\CMORE_Box_Keypoints\\data\\train\\images\\IMG_1156_frame_000049.png: 288x512 1 box, 16.5ms\n",
      "Speed: 0.7ms preprocess, 16.5ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 512)\n",
      "Evaluated 50 images.\n",
      "\n",
      "Overall Dataset Metrics:\n",
      "Total Images Evaluated: 50\n",
      "Failed Images: 0\n",
      "  MSE: 0.000165\n",
      "  MAE: 0.009392\n",
      "  PCK@0.02: 0.7020\n",
      "  PCK@0.05: 1.0000\n",
      "  PCK@0.10: 1.0000\n",
      "\n",
      "Average Per-Keypoint Metrics:\n",
      "  Keypoint     Avg Distance    Avg MSE         PCK@0.05\n",
      "  ------------------------------------------------------------\n",
      "  FrontTopL    0.003339        0.000006        1.0000\n",
      "  FrontBotL    0.010794        0.000058        1.0000\n",
      "  FrontTopM    0.011896        0.000071        1.0000\n",
      "  FrontBotM    0.007027        0.000025        1.0000\n",
      "  FrontTopR    0.009677        0.000047        1.0000\n",
      "  FrontBotR    0.023183        0.000269        1.0000\n",
      "  BackDivTop   0.012837        0.000083        1.0000\n",
      "  FrontDivTop  0.022051        0.000243        1.0000\n",
      "  BackTopL     0.018228        0.000167        1.0000\n",
      "  BackTopR     0.036939        0.000682        1.0000\n",
      "\n",
      "======================================================================\n",
      "CPU RESOURCE BENCHMARK (End-to-End)\n",
      "======================================================================\n",
      "Baseline Process RAM (Model Loaded): 2324.44 MB\n",
      "\n",
      "Warming up (5 iterations)...\n",
      "Benchmarking (50 iterations)...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "SUMMARY RESULTS\n",
      "\n",
      "Avg Latency (E2E):  37.58 ms\n",
      "FPS:                26.61\n",
      "CPU Usage (Avg):    103.3%\n",
      "\n",
      "Baseline RAM:       2324.44 MB\n",
      "Peak RAM:           2324.44 MB (CRITICAL LIMIT)\n",
      "Max RAM Spike:      +0.00 MB\n",
      "\n",
      "Saved evaluation results to: evaluation_results_20251121_152724.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mse_overall': 0.00016508917068535245,\n",
       " 'mae_overall': 0.00939194630614042,\n",
       " 'pck_0.02': 0.7020000000000002,\n",
       " 'pck_0.05': 1.0,\n",
       " 'pck_0.1': 1.0,\n",
       " 'avg_distances_per_keypoint': [0.003339289216485081,\n",
       "  0.010793506824754858,\n",
       "  0.011895563066887868,\n",
       "  0.007026570620736547,\n",
       "  0.009676984471908788,\n",
       "  0.02318293226973189,\n",
       "  0.012837331289792664,\n",
       "  0.022050583448786833,\n",
       "  0.018228111477448093,\n",
       "  0.036939205618616015],\n",
       " 'avg_mse_per_keypoint': [5.616007876053586e-06,\n",
       "  5.828013948180556e-05,\n",
       "  7.081844355377459e-05,\n",
       "  2.470319262406303e-05,\n",
       "  4.734053116351326e-05,\n",
       "  0.00026880501941945877,\n",
       "  8.294876261324715e-05,\n",
       "  0.00024333805431136417,\n",
       "  0.00016667773906525632,\n",
       "  0.0006823638167449878],\n",
       " 'avg_pck_per_keypoint': {'pck_0.02': [1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.02,\n",
       "   1.0,\n",
       "   0.0],\n",
       "  'pck_0.05': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "  'pck_0.1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]},\n",
       " 'num_images': 50,\n",
       " 'num_failed': 0,\n",
       " 'model_path': './/best.pt',\n",
       " 'test_images_dir': 'data\\\\train\\\\images',\n",
       " 'test_labels_dir': 'data\\\\train\\\\labels',\n",
       " 'timestamp': '2025-11-21T15:27:22.690981',\n",
       " 'resource_usage': {'avg_latency_ms': 37.57809200040356,\n",
       "  'p95_latency_ms': 40.92570500015427,\n",
       "  'fps': 26.61124998015495,\n",
       "  'baseline_ram_mb': 2324.44140625,\n",
       "  'peak_ram_mb': 2324.44140625,\n",
       "  'ram_spike_mb': 0.0,\n",
       "  'avg_cpu_usage_percent': 103.3,\n",
       "  'num_samples': 50}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import json\n",
    "from datetime import datetime\n",
    "    \n",
    "print(\"EVALUATE ALL TEST IMAGES\")\n",
    "\n",
    "def evaluate_dataset(model_path: str, test_images_dir: str, test_labels_dir: str,\n",
    "                    save_results: bool = True, output_path: str = None, benchmark_resources: bool = True, \n",
    "                    num_benchmark_iterations: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate model on entire test dataset\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to YOLO model (.pt file)\n",
    "        test_images_dir: Directory with test images\n",
    "        test_labels_dir: Directory with test labels (.txt files)\n",
    "        save_results: Whether to save results to JSON\n",
    "        output_path: Where to save results (optional)\n",
    "        benchmark_resources: Whether to run resource usage benchmark\n",
    "        num_benchmark_iterations: Number of iterations for resource benchmark\n",
    "    Returns:\n",
    "        dict with aggregated metrics across all images\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    test_images_dir = Path(test_images_dir)\n",
    "    test_labels_dir = Path(test_labels_dir)\n",
    "    \n",
    "    test_images = []\n",
    "    for img_file in test_images_dir.glob('*'):\n",
    "        test_images.append(img_file)\n",
    "    test_images = test_images[:50]\n",
    "    print(f\"Found {len(test_images)} test images.\")\n",
    "    \n",
    "    images_with_labels = []\n",
    "    for img_path in test_images:\n",
    "        label_path = test_labels_dir / (img_path.stem + '.txt')\n",
    "        if label_path.exists():\n",
    "            images_with_labels.append((img_path, label_path))\n",
    "        else:\n",
    "            print(f\"Warning: No label file for image {img_path.name}, skipping.\")\n",
    "            \n",
    "    print(f\"Evaluating {len(images_with_labels)} images with labels.\")\n",
    "    \n",
    "    test_images = images_with_labels\n",
    "    all_metrics = []\n",
    "    all_distances = []\n",
    "    all_mse_per_kpt = []\n",
    "    all_pck_per_kpt = {\n",
    "        'pck_0.02': [],\n",
    "        'pck_0.05': [],\n",
    "        'pck_0.1': []\n",
    "    }\n",
    "    \n",
    "    failed_images = []\n",
    "    \n",
    "    for idx, (img_path, label_path) in enumerate(test_images):\n",
    "        if (idx + 1) % 25 == 0:\n",
    "            print(f\"  Progress: {idx + 1}/{len(test_images)}\")\n",
    "                \n",
    "        try:\n",
    "            gt_data = parse_yolo_label_file(str(label_path))\n",
    "            \n",
    "            pred_data = get_yolo_predictions(model, str(img_path), normalize=True)\n",
    "            \n",
    "            metrics = evaluate_single_frame(\n",
    "                pred_data['keypoints_xy'],\n",
    "                gt_data['keypoints_xy'],\n",
    "                gt_data['visibility'],\n",
    "                thresholds=[0.02, 0.05, 0.10]\n",
    "            )\n",
    "            \n",
    "            all_metrics.append({\n",
    "                'image': img_path.name,\n",
    "                'mse': metrics['mse_overall'],\n",
    "                'mae': metrics['mae_overall'],\n",
    "                'pck_0.02': metrics.get('pck_0.02', 0.0),\n",
    "                'pck_0.05': metrics.get('pck_0.05', 0.0),\n",
    "                'pck_0.1': metrics.get('pck_0.1', 0.0),\n",
    "                'confidence': pred_data['confidence']\n",
    "            })\n",
    "            all_distances.append(metrics['distances'])\n",
    "            all_mse_per_kpt.append(metrics['mse_per_keypoint'])\n",
    "            \n",
    "            all_pck_per_kpt['pck_0.02'].append(metrics['pck_0.02_per_keypoint'])\n",
    "            all_pck_per_kpt['pck_0.05'].append(metrics['pck_0.05_per_keypoint'])\n",
    "            all_pck_per_kpt['pck_0.1'].append(metrics['pck_0.1_per_keypoint'])\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Failed on {img_path.name}: {e}\")\n",
    "            failed_images.append(str(img_path.name))\n",
    "            \n",
    "    print(f\"Evaluated {len(all_metrics)} images.\")\n",
    "    \n",
    "    if failed_images:\n",
    "        print(f\"Failed on {len(failed_images)} images: {failed_images}\")\n",
    "            \n",
    "    # Aggregate overall metrics\n",
    "    all_distances = np.array(all_distances)\n",
    "    all_mse_per_kpt = np.array(all_mse_per_kpt)\n",
    "    \n",
    "    overall_metrics = {\n",
    "        'mse_overall': float(np.mean([m['mse'] for m in all_metrics])),\n",
    "        'mae_overall': float(np.mean([m['mae'] for m in all_metrics])),\n",
    "        'pck_0.02': float(np.mean([m['pck_0.02'] for m in all_metrics])),\n",
    "        'pck_0.05': float(np.mean([m['pck_0.05'] for m in all_metrics])),\n",
    "        'pck_0.1': float(np.mean([m['pck_0.1'] for m in all_metrics])),\n",
    "        'avg_distances_per_keypoint': np.mean(all_distances, axis=0).tolist(),\n",
    "        'avg_mse_per_keypoint': np.mean(all_mse_per_kpt, axis=0).tolist(),\n",
    "        'avg_pck_per_keypoint': {\n",
    "            'pck_0.02': np.mean(all_pck_per_kpt['pck_0.02'], axis=0).tolist(),\n",
    "            'pck_0.05': np.mean(all_pck_per_kpt['pck_0.05'], axis=0).tolist(),\n",
    "            'pck_0.1': np.mean(all_pck_per_kpt['pck_0.1'], axis=0).tolist(),\n",
    "        },\n",
    "        'num_images': len(all_metrics),\n",
    "        'num_failed': len(failed_images),\n",
    "        'model_path': str(model_path),\n",
    "        'test_images_dir': str(test_images_dir),\n",
    "        'test_labels_dir': str(test_labels_dir),\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    }\n",
    "    \n",
    "    print(\"\\nOverall Dataset Metrics:\")\n",
    "    print(f\"Total Images Evaluated: {overall_metrics['num_images']}\")\n",
    "    print(f\"Failed Images: {overall_metrics['num_failed']}\")\n",
    "    print(f\"  MSE: {overall_metrics['mse_overall']:.6f}\")\n",
    "    print(f\"  MAE: {overall_metrics['mae_overall']:.6f}\")\n",
    "    print(f\"  PCK@0.02: {overall_metrics['pck_0.02']:.4f}\")\n",
    "    print(f\"  PCK@0.05: {overall_metrics['pck_0.05']:.4f}\")\n",
    "    print(f\"  PCK@0.10: {overall_metrics['pck_0.1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nAverage Per-Keypoint Metrics:\")\n",
    "    keypoint_names = [\n",
    "        'FrontTopL', 'FrontBotL', 'FrontTopM', 'FrontBotM',\n",
    "        'FrontTopR', 'FrontBotR', 'BackDivTop', 'FrontDivTop',\n",
    "        'BackTopL', 'BackTopR'\n",
    "    ]\n",
    "    print(f\"  {'Keypoint':<12} {'Avg Distance':<15} {'Avg MSE':<15} {'PCK@0.05'}\")\n",
    "    print(f\"  {'-'*60}\")\n",
    "    for i, name in enumerate(keypoint_names):\n",
    "        avg_dist = overall_metrics['avg_distances_per_keypoint'][i]\n",
    "        avg_mse = overall_metrics['avg_mse_per_keypoint'][i]\n",
    "        pck_05 = overall_metrics['avg_pck_per_keypoint']['pck_0.05'][i]\n",
    "        print(f\"  {name:<12} {avg_dist:<15.6f} {avg_mse:<15.6f} {pck_05:.4f}\")\n",
    "        \n",
    "    if benchmark_resources:\n",
    "        resource_metrics = benchmark_resource_usage(\n",
    "            model, \n",
    "            test_images, \n",
    "            num_warmup=5,\n",
    "            num_iterations=min(num_benchmark_iterations, len(test_images))\n",
    "        )\n",
    "        overall_metrics['resource_usage'] = resource_metrics\n",
    "    # Save results to JSON\n",
    "    if save_results:\n",
    "        if output_path is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_path = f\"evaluation_results_{timestamp}.json\"\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(overall_metrics, f, indent=4)\n",
    "        \n",
    "        print(f\"\\nSaved evaluation results to: {output_path}\")\n",
    "        \n",
    "    return overall_metrics\n",
    "\n",
    "# Example usage:\n",
    "evaluate_dataset(\n",
    "    model_path='.//best.pt', # model is from https://github.com/AlundorZhu/CMORE/blob/24d8e02045004bb753b986f82fdf974e70d14637/playground/keypoints/runs/pose/train/weights/best.pt\n",
    "    test_images_dir='data/train/images',\n",
    "    test_labels_dir='data/train/labels',\n",
    "    save_results=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evs271_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
